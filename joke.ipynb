{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using BERT-style models via Huggingface for joke generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "set_seed(42)\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 662/662 [00:00<00:00, 350kB/s]\n",
      "Downloading tf_model.h5: 100%|██████████| 3.40G/3.40G [01:20<00:00, 42.3MB/s]\n",
      "/home/ucloud/.local/lib/python3.10/site-packages/keras/initializers/initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n",
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at google/flan-t5-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "Downloading (…)neration_config.json: 100%|██████████| 147/147 [00:00<00:00, 269kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 2.54k/2.54k [00:00<00:00, 4.72MB/s]\n",
      "Downloading spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 4.00MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 2.42M/2.42M [00:01<00:00, 1.42MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<00:00, 6.90MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    I'm tired of following my dreams. I'm just goi...\n",
      "1    Did you hear about the guy whose whole left si...\n",
      "2    Why didn’t the skeleton cross the road? Becaus...\n",
      "3    What did one nut say as he chased another nut?...\n",
      "4    Where do fish keep their money? In the riverba...\n",
      "5    I accidentally took my cats meds last night. D...\n",
      "6    Chances are if you' ve seen one shopping cente...\n",
      "7    Dermatologists are always in a hurry. They spe...\n",
      "8    I knew I shouldn't steal a mixer from work, bu...\n",
      "9    I won an argument with a weather forecaster on...\n",
      "Name: Joke, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#prompts\n",
    "\n",
    "#load in data\n",
    "file = os.path.join(os.getcwd(), \"data\", \"dad-a-base.csv\")\n",
    "data = pd.read_csv(file)\n",
    "\n",
    "\n",
    "print(data[\"Joke\"].iloc[0:10])\n",
    "\n",
    "#text prompt\n",
    "prompts = data[\"Joke\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joke_generator(input_text:str) -> str:\n",
    "    prompt = f\"Make a dad joke from the following text: {input_text}\"\n",
    "    inputs = tokenizer(prompt, \n",
    "                    max_length = 200,\n",
    "                    return_tensors=\"tf\")\n",
    "    outputs = model.generate(**inputs)\n",
    "    print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"i'm gonna kill you\"]\n"
     ]
    }
   ],
   "source": [
    "joke_generator(\"snake\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
